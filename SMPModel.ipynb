{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "from LoadDataset import CustomImageDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from config import dataset_test_path, dataset_train_path, dataset_val_path\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomImageDataset(file_path=dataset_train_path, folder_path=dataset_root, transform=transform)\n",
    "val_dataset = CustomImageDataset(file_path=dataset_val_path, folder_path=dataset_root, transform=transform)\n",
    "test_dataset = CustomImageDataset(file_path=dataset_test_path, folder_path=dataset_root, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(674373, 207499, 155625, 'total:', 1037497)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset), len(test_dataset), \"total:\", len(train_dataset) + len(val_dataset) + len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, num_classes=14):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "        # TODO: research if there are other weights to be set (like V2 if it exists)\n",
    "        num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNet50(num_classes=len(class_names))\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Prototype Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Procedure\n",
    "1. Calculate cosine similarity $S_{i j}$ of the data points' deep features\n",
    "2. Compute the density of each image $\\rho_i$ by counting the number of images with a similarity exceeding the threshold \n",
    "3. Calculate how diverse and representative an image is based on its density and cosine similarity resulting in an overall score $\\eta_i$\n",
    "4. Select the highest scoring images as prototypes\n",
    "\n",
    "\n",
    "##### Background Theory About Prototypes\n",
    "\n",
    "determine high **density** of data points $\\rho_i$ (#samples similar to sample) because they are likely to have correct labels, making them good candidates for prototypes\n",
    "identify similar data points ($i$ and $j$) using their **cosine similarity** $S_{ij}$ and a **threshold** value $S_c$ So $\\operatorname{sign}\\left(S_{i j}-S_c\\right)$ returns 1 if the threshold is exceeded (else 0): \n",
    "$S_{i j}=\\frac{\\mathcal{G}\\left(\\mathbf{x}_i\\right)^T \\mathcal{G}\\left(\\mathbf{x}_j\\right)}{\\left\\|\\mathcal{G}\\left(\\mathbf{x}_i\\right)\\right\\|_2\\left\\|\\mathcal{G}\\left(\\mathbf{x}_j\\right)\\right\\|_2}$ with the Euclidean norm (length) of the deep features of sample $i$ $\\|\\mathcal{G}\\left(\\mathbf{x}_i)\\right\\|_2$\n",
    "\n",
    "$\\rho_i=\\sum_{j=1}^m \\operatorname{sign}\\left(S_{i j}-S_c\\right)$\n",
    "\n",
    "**select** diverse and representative prototypes with $\\eta_i$, ensuring that the chosen prototypes cover a wide range of characteristics within a class, which improves the accuracy and robustness of the label correction process\n",
    "\n",
    "if the density of a single data point $i$ is **smaller** than the maximum density ($\\rho_i<\\rho_{\\max }$), we look at the **maximum** **similarity** between $i$ and the denser data point $j$ to identify how well $i$ is connected to a denser and more reliable feature space. This $\\eta_i$ indicates, if the data sample is a **representative** prototype.\n",
    "\n",
    "if the data point $i$ has the **highest density**, we need to check that the prototypes are **not too similar**. Thus, we take the smallest similarity value to select prototypes that are **spread out** in the feature space to avoid redundancy and ensure **diversity**.\n",
    "\n",
    "$\\eta_i= \\begin{cases}\\max_{j, \\rho_j>\\rho_i} S_{i j}, & \\rho_i<\\rho_{\\max } \\\\ \\min_j S_{i j}, & \\rho_i=\\rho_{\\max }\\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_prototypes(features, labels, num_prototypes=num_prototypes, similarity_threshold=similarity_threshold):\n",
    "    prototypes = {}\n",
    "    for label in np.unique(labels):  # Loop through each unique label\n",
    "        class_features = features[labels == label]  # Extract features for the current label\n",
    "        # Calculate cosine similarity matrix\n",
    "        norm_class_features = class_features / np.linalg.norm(class_features, axis=1, keepdims=True)\n",
    "        similarity_matrix = np.matmul(norm_class_features, norm_class_features.T)\n",
    "        # Calculate densities (rho) based on similarity threshold\n",
    "        densities = np.sum(similarity_matrix > similarity_threshold, axis=1)\n",
    "        max_density = np.max(densities)\n",
    "        \n",
    "        # Calculate eta to ensure diversity and representativeness\n",
    "        eta = np.full(len(densities), float('inf'))\n",
    "        for i in range(len(densities)):\n",
    "            if densities[i] < max_density:\n",
    "                eta[i] = np.max(similarity_matrix[i][densities > densities[i]])\n",
    "            else:\n",
    "                eta[i] = np.min(similarity_matrix[i][densities == densities[i]])\n",
    "        \n",
    "        # Select the highest scoring prototypes\n",
    "        selected_prototypes = []\n",
    "        while len(selected_prototypes) < num_prototypes:\n",
    "            candidate_indices = np.where(eta < 0.95)[0]\n",
    "            if len(candidate_indices) == 0:\n",
    "                break\n",
    "            best_candidate = candidate_indices[np.argmax(densities[candidate_indices])]\n",
    "            selected_prototypes.append(best_candidate)\n",
    "            eta[best_candidate] = float('inf')  # Ensure this candidate is not selected again\n",
    "        \n",
    "        prototypes[label] = class_features[selected_prototypes[:num_prototypes]]\n",
    "    return prototypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Label Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Procedure\n",
    "1. Determine the highest average similarity score $\\sigma_c$ for each image based on the deep features of the class prototypes \n",
    "2. Assign the corresponding prototype’s label as the corrected, new pseudo label $\\hat{y}$\n",
    "3. Start new iteration (with new training and label correction)\n",
    "\n",
    "##### Background Theory About Label Correction\n",
    "determine **similarity** of data instance to prototypes with the average similarity score for class $c$ $\\sigma_c$: $\\sigma_c=\\frac{1}{p} \\sum_{l=1}^p \\cos \\left(\\mathcal{G}(\\mathbf{x}), \\mathcal{G}\\left(\\mathbf{x}_{c l}\\right)\\right), c=1 \\ldots K$\n",
    "\n",
    "($p$ := #prototypes for each class) \n",
    "\n",
    "cosine similarity between the deep features of the sample $\\mathbf{x}$ and the prototype $\\mathbf{x}_{c l}$)\n",
    "\n",
    "select the class $c$ that has the highest average similarity score $\\sigma_c$: \n",
    "$\\hat{y}=\\operatorname{argmax}_c \\sigma_c, c=1 \\ldots K$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_labels(features, prototypes):\n",
    "    corrected_labels = []\n",
    "    for feature in features:  # Iterate through each feature vector\n",
    "        max_similarity = -1\n",
    "        corrected_label = -1\n",
    "        for label, proto_features in prototypes.items():  # Iterate through each class's prototypes\n",
    "            # Calculate mean similarity of the feature to the class's prototypes\n",
    "            similarities = np.mean([np.dot(feature, proto_feature) for proto_feature in proto_features])\n",
    "            if similarities > max_similarity:  # Update if current similarity is the highest\n",
    "                max_similarity = similarities\n",
    "                corrected_label = label\n",
    "        corrected_labels.append(corrected_label)  # Append the most similar class's label\n",
    "    return np.array(corrected_labels)  # Return the corrected labels as a NumPy array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Training Loop with Iterative Self-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loss Function\n",
    "1st iteration: cross-entropy loss to quantify how well the model's predictions match the true labels: $\\mathcal{L}(\\mathcal{F}(\\theta, \\mathbf{x}), y)=-\\frac{1}{n} \\sum_{i=1}^n \\log \\left(\\mathcal{F}\\left(\\theta, \\mathbf{x}_i\\right)_{y_i}\\right)$\n",
    "\n",
    "we use this because it is a normalized measure of performance that is independent of the batch size or dataset size (unlike the total loss) → easier to interpret and compare the model's effectiveness\n",
    "\n",
    "In the **subsequent iterations** we need to **combine the losses** of the original noisy $y$ and corrected labels $\\hat{y}$ with the weight factor $\\alpha$ for training: \n",
    "$\\mathcal{L}_{\\text {total }}=(1-\\alpha) \\mathcal{L}(\\mathcal{F}(\\theta, \\mathbf{x}), y)+\\alpha \\mathcal{L}(\\mathcal{F}(\\theta, \\mathbf{x}), \\hat{y})$\n",
    "\n",
    "##### Parameters\n",
    "\n",
    "1st iteration: We are looking for the **optimal parameters** $\\theta^*$ (weights and biases) by **minimizing the loss function** (truth value $Y$ and prediction  $\\mathcal{F}(\\theta, \\mathbf{X})$: $\\theta^*=\\operatorname{argmin}_\\theta \\mathcal{L}(Y, \\mathcal{F}(\\theta, \\mathbf{X}))$\n",
    "\n",
    "In the **subsequent iterations** **update** the optimal parameters because of the corrected labels $\\hat{Y}$ using the prototypes $\\mathbf{X}_s$: $\\theta^*=\\operatorname{argmin}_\\theta \\mathcal{L}\\left(Y, \\hat{Y}\\left(\\mathbf{X}, \\mathbf{X}_s\\right), \\mathcal{F}(\\theta, \\mathbf{X})\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 16:59:20.718151: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "def train_model(model, train_loader, num_epochs=num_epochs, learning_rate=lr, momentum=momentum, weight_decay=weight_decay, step_size=step_size, gamma=gamma, alpha=alpha, num_prototypes=num_prototypes, device=device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        # Initialize progress bar for the epoch\n",
    "        progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch+1}/{num_epochs}', ncols=100)\n",
    "        \n",
    "        for i, (inputs, labels) in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            if epoch == 0:  # First iteration\n",
    "                loss = criterion(outputs, labels)\n",
    "            else:  # Subsequent iterations\n",
    "                corrected = correct_labels(all_features, prototypes)\n",
    "                corrected = torch.tensor(corrected).to(device)\n",
    "                loss = (1 - alpha) * criterion(outputs, labels) + alpha * criterion(outputs, corrected)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Update progress bar with batch loss\n",
    "            progress_bar.set_postfix(batch_loss=loss.item())\n",
    "        \n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        writer.add_scalar('Loss/train', avg_loss, epoch)\n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            model.eval()\n",
    "            all_features = []\n",
    "            all_labels = []\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in train_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    features = model(inputs)\n",
    "                    all_features.append(features.cpu().numpy())\n",
    "                    all_labels.append(labels.cpu().numpy())\n",
    "            \n",
    "            all_features = np.concatenate(all_features)\n",
    "            all_labels = np.concatenate(all_labels)\n",
    "            \n",
    "            prototypes = select_prototypes(all_features, all_labels, num_prototypes=num_prototypes, similarity_threshold=similarity_threshold)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "        total_elapsed_time = time.time() - total_start_time\n",
    "        estimated_remaining_time = (num_epochs - epoch - 1) * (total_elapsed_time / (epoch + 1))\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Time per Epoch: {epoch_duration:.2f}s, Estimated Time Remaining: {estimated_remaining_time:.2f}s')\n",
    "    \n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15:   0%|                                                          | 0/5269 [00:17<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 26\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, num_epochs, learning_rate, momentum, weight_decay, step_size, gamma, alpha, num_prototypes, device)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Initialize progress bar for the epoch\u001b[39;00m\n\u001b[1;32m     24\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_loader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[1;32m     27\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     28\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:262\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:429\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 429\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:936\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    933\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
