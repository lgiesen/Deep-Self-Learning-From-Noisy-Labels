digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	5029688032 [label="
 (1, 14)" fillcolor=darkolivegreen1]
	5028707296 [label=AddmmBackward0]
	5028707824 -> 5028707296
	5029529056 [label="fc.bias
 (14)" fillcolor=lightblue]
	5029529056 -> 5028707824
	5028707824 [label=AccumulateGrad]
	5028707536 -> 5028707296
	5028707536 [label=ReshapeAliasBackward0]
	5028704560 -> 5028707536
	5028704560 [label=MeanBackward1]
	5028697936 -> 5028704560
	5028697936 [label=ReluBackward0]
	5028701104 -> 5028697936
	5028701104 [label=AddBackward0]
	5028700720 -> 5028701104
	5028700720 [label=NativeBatchNormBackward0]
	5028697648 -> 5028700720
	5028697648 [label=ConvolutionBackward0]
	5028696976 -> 5028697648
	5028696976 [label=ReluBackward0]
	5028698368 -> 5028696976
	5028698368 [label=NativeBatchNormBackward0]
	5028698224 -> 5028698368
	5028698224 [label=ConvolutionBackward0]
	5028699664 -> 5028698224
	5028699664 [label=ReluBackward0]
	5028702544 -> 5028699664
	5028702544 [label=NativeBatchNormBackward0]
	5028702736 -> 5028702544
	5028702736 [label=ConvolutionBackward0]
	5028700816 -> 5028702736
	5028700816 [label=ReluBackward0]
	5028702928 -> 5028700816
	5028702928 [label=AddBackward0]
	5028702064 -> 5028702928
	5028702064 [label=NativeBatchNormBackward0]
	5028702112 -> 5028702064
	5028702112 [label=ConvolutionBackward0]
	5028697744 -> 5028702112
	5028697744 [label=ReluBackward0]
	5028697264 -> 5028697744
	5028697264 [label=NativeBatchNormBackward0]
	5028697408 -> 5028697264
	5028697408 [label=ConvolutionBackward0]
	5028697600 -> 5028697408
	5028697600 [label=ReluBackward0]
	5028698416 -> 5028697600
	5028698416 [label=NativeBatchNormBackward0]
	5028698608 -> 5028698416
	5028698608 [label=ConvolutionBackward0]
	5028703216 -> 5028698608
	5028703216 [label=ReluBackward0]
	5028701392 -> 5028703216
	5028701392 [label=AddBackward0]
	5028701536 -> 5028701392
	5028701536 [label=NativeBatchNormBackward0]
	5028701152 -> 5028701536
	5028701152 [label=ConvolutionBackward0]
	5028700912 -> 5028701152
	5028700912 [label=ReluBackward0]
	5028701680 -> 5028700912
	5028701680 [label=NativeBatchNormBackward0]
	5028701728 -> 5028701680
	5028701728 [label=ConvolutionBackward0]
	5028705520 -> 5028701728
	5028705520 [label=ReluBackward0]
	5028706624 -> 5028705520
	5028706624 [label=NativeBatchNormBackward0]
	5028706576 -> 5028706624
	5028706576 [label=ConvolutionBackward0]
	5028704464 -> 5028706576
	5028704464 [label=ReluBackward0]
	5028704848 -> 5028704464
	5028704848 [label=AddBackward0]
	5028704320 -> 5028704848
	5028704320 [label=NativeBatchNormBackward0]
	5028705088 -> 5028704320
	5028705088 [label=ConvolutionBackward0]
	5028704608 -> 5028705088
	5028704608 [label=ReluBackward0]
	5028698080 -> 5028704608
	5028698080 [label=NativeBatchNormBackward0]
	5028699760 -> 5028698080
	5028699760 [label=ConvolutionBackward0]
	5028699856 -> 5028699760
	5028699856 [label=ReluBackward0]
	5028700096 -> 5028699856
	5028700096 [label=NativeBatchNormBackward0]
	5028700336 -> 5028700096
	5028700336 [label=ConvolutionBackward0]
	5028704416 -> 5028700336
	5028704416 [label=ReluBackward0]
	5028696208 -> 5028704416
	5028696208 [label=AddBackward0]
	5028699136 -> 5028696208
	5028699136 [label=NativeBatchNormBackward0]
	5028699088 -> 5028699136
	5028699088 [label=ConvolutionBackward0]
	5028699232 -> 5028699088
	5028699232 [label=ReluBackward0]
	5028703552 -> 5028699232
	5028703552 [label=NativeBatchNormBackward0]
	5028696640 -> 5028703552
	5028696640 [label=ConvolutionBackward0]
	5028703936 -> 5028696640
	5028703936 [label=ReluBackward0]
	5028703840 -> 5028703936
	5028703840 [label=NativeBatchNormBackward0]
	5028703696 -> 5028703840
	5028703696 [label=ConvolutionBackward0]
	5028696160 -> 5028703696
	5028696160 [label=ReluBackward0]
	5028705856 -> 5028696160
	5028705856 [label=AddBackward0]
	5028707248 -> 5028705856
	5028707248 [label=NativeBatchNormBackward0]
	5028707200 -> 5028707248
	5028707200 [label=ConvolutionBackward0]
	5028707104 -> 5028707200
	5028707104 [label=ReluBackward0]
	5028707008 -> 5028707104
	5028707008 [label=NativeBatchNormBackward0]
	5028706000 -> 5028707008
	5028706000 [label=ConvolutionBackward0]
	5028705808 -> 5028706000
	5028705808 [label=ReluBackward0]
	5029994288 -> 5028705808
	5029994288 [label=NativeBatchNormBackward0]
	5029994000 -> 5029994288
	5029994000 [label=ConvolutionBackward0]
	5028707152 -> 5029994000
	5028707152 [label=ReluBackward0]
	5029993472 -> 5028707152
	5029993472 [label=AddBackward0]
	5029993232 -> 5029993472
	5029993232 [label=NativeBatchNormBackward0]
	5029996448 -> 5029993232
	5029996448 [label=ConvolutionBackward0]
	5029994432 -> 5029996448
	5029994432 [label=ReluBackward0]
	5029994864 -> 5029994432
	5029994864 [label=NativeBatchNormBackward0]
	5029994768 -> 5029994864
	5029994768 [label=ConvolutionBackward0]
	5029994720 -> 5029994768
	5029994720 [label=ReluBackward0]
	5029993808 -> 5029994720
	5029993808 [label=NativeBatchNormBackward0]
	5029993616 -> 5029993808
	5029993616 [label=ConvolutionBackward0]
	5029993376 -> 5029993616
	5029993376 [label=ReluBackward0]
	5029992656 -> 5029993376
	5029992656 [label=AddBackward0]
	5029992704 -> 5029992656
	5029992704 [label=NativeBatchNormBackward0]
	5029992416 -> 5029992704
	5029992416 [label=ConvolutionBackward0]
	5029992464 -> 5029992416
	5029992464 [label=ReluBackward0]
	5029992368 -> 5029992464
	5029992368 [label=NativeBatchNormBackward0]
	5029992128 -> 5029992368
	5029992128 [label=ConvolutionBackward0]
	5029988912 -> 5029992128
	5029988912 [label=ReluBackward0]
	5029992032 -> 5029988912
	5029992032 [label=NativeBatchNormBackward0]
	5029991792 -> 5029992032
	5029991792 [label=ConvolutionBackward0]
	5029992800 -> 5029991792
	5029992800 [label=ReluBackward0]
	5029989200 -> 5029992800
	5029989200 [label=AddBackward0]
	5029989152 -> 5029989200
	5029989152 [label=NativeBatchNormBackward0]
	5029989680 -> 5029989152
	5029989680 [label=ConvolutionBackward0]
	5029989584 -> 5029989680
	5029989584 [label=ReluBackward0]
	5029989392 -> 5029989584
	5029989392 [label=NativeBatchNormBackward0]
	5029989968 -> 5029989392
	5029989968 [label=ConvolutionBackward0]
	5029990016 -> 5029989968
	5029990016 [label=ReluBackward0]
	5029990112 -> 5029990016
	5029990112 [label=NativeBatchNormBackward0]
	5029991504 -> 5029990112
	5029991504 [label=ConvolutionBackward0]
	5029990736 -> 5029991504
	5029990736 [label=ReluBackward0]
	5029990592 -> 5029990736
	5029990592 [label=AddBackward0]
	5029990544 -> 5029990592
	5029990544 [label=NativeBatchNormBackward0]
	5029990784 -> 5029990544
	5029990784 [label=ConvolutionBackward0]
	5029991264 -> 5029990784
	5029991264 [label=ReluBackward0]
	5029991120 -> 5029991264
	5029991120 [label=NativeBatchNormBackward0]
	5029991456 -> 5029991120
	5029991456 [label=ConvolutionBackward0]
	5029991744 -> 5029991456
	5029991744 [label=ReluBackward0]
	5029991600 -> 5029991744
	5029991600 [label=NativeBatchNormBackward0]
	5029991840 -> 5029991600
	5029991840 [label=ConvolutionBackward0]
	5029990496 -> 5029991840
	5029990496 [label=ReluBackward0]
	5029992176 -> 5029990496
	5029992176 [label=AddBackward0]
	5029988528 -> 5029992176
	5029988528 [label=NativeBatchNormBackward0]
	5029987472 -> 5029988528
	5029987472 [label=ConvolutionBackward0]
	5029987184 -> 5029987472
	5029987184 [label=ReluBackward0]
	5029986752 -> 5029987184
	5029986752 [label=NativeBatchNormBackward0]
	5029987040 -> 5029986752
	5029987040 [label=ConvolutionBackward0]
	5029986848 -> 5029987040
	5029986848 [label=ReluBackward0]
	5029987520 -> 5029986848
	5029987520 [label=NativeBatchNormBackward0]
	5029988096 -> 5029987520
	5029988096 [label=ConvolutionBackward0]
	5029988672 -> 5029988096
	5029988672 [label=ReluBackward0]
	5029987808 -> 5029988672
	5029987808 [label=AddBackward0]
	5029987712 -> 5029987808
	5029987712 [label=NativeBatchNormBackward0]
	5029988336 -> 5029987712
	5029988336 [label=ConvolutionBackward0]
	5029988144 -> 5029988336
	5029988144 [label=ReluBackward0]
	5029986656 -> 5029988144
	5029986656 [label=NativeBatchNormBackward0]
	5029986608 -> 5029986656
	5029986608 [label=ConvolutionBackward0]
	5029993328 -> 5029986608
	5029993328 [label=ReluBackward0]
	5029996544 -> 5029993328
	5029996544 [label=NativeBatchNormBackward0]
	5029993184 -> 5029996544
	5029993184 [label=ConvolutionBackward0]
	5029987952 -> 5029993184
	5029987952 [label=ReluBackward0]
	5029997024 -> 5029987952
	5029997024 [label=AddBackward0]
	5029996928 -> 5029997024
	5029996928 [label=NativeBatchNormBackward0]
	5029996784 -> 5029996928
	5029996784 [label=ConvolutionBackward0]
	5029996592 -> 5029996784
	5029996592 [label=ReluBackward0]
	5029996304 -> 5029996592
	5029996304 [label=NativeBatchNormBackward0]
	5029996208 -> 5029996304
	5029996208 [label=ConvolutionBackward0]
	5029995920 -> 5029996208
	5029995920 [label=ReluBackward0]
	5029995776 -> 5029995920
	5029995776 [label=NativeBatchNormBackward0]
	5029995632 -> 5029995776
	5029995632 [label=ConvolutionBackward0]
	5029997408 -> 5029995632
	5029997408 [label=ReluBackward0]
	5029997552 -> 5029997408
	5029997552 [label=AddBackward0]
	5029997648 -> 5029997552
	5029997648 [label=NativeBatchNormBackward0]
	5029997792 -> 5029997648
	5029997792 [label=ConvolutionBackward0]
	5029997984 -> 5029997792
	5029997984 [label=ReluBackward0]
	5029998128 -> 5029997984
	5029998128 [label=NativeBatchNormBackward0]
	5029998224 -> 5029998128
	5029998224 [label=ConvolutionBackward0]
	5029998416 -> 5029998224
	5029998416 [label=ReluBackward0]
	5029998560 -> 5029998416
	5029998560 [label=NativeBatchNormBackward0]
	5029998656 -> 5029998560
	5029998656 [label=ConvolutionBackward0]
	5029997600 -> 5029998656
	5029997600 [label=ReluBackward0]
	5029998944 -> 5029997600
	5029998944 [label=AddBackward0]
	5029999040 -> 5029998944
	5029999040 [label=NativeBatchNormBackward0]
	5029999184 -> 5029999040
	5029999184 [label=ConvolutionBackward0]
	5029999376 -> 5029999184
	5029999376 [label=ReluBackward0]
	5029999520 -> 5029999376
	5029999520 [label=NativeBatchNormBackward0]
	5029999616 -> 5029999520
	5029999616 [label=ConvolutionBackward0]
	5029999808 -> 5029999616
	5029999808 [label=ReluBackward0]
	5029999952 -> 5029999808
	5029999952 [label=NativeBatchNormBackward0]
	5030000048 -> 5029999952
	5030000048 [label=ConvolutionBackward0]
	5029998992 -> 5030000048
	5029998992 [label=ReluBackward0]
	5030000336 -> 5029998992
	5030000336 [label=AddBackward0]
	5030000432 -> 5030000336
	5030000432 [label=NativeBatchNormBackward0]
	5030000576 -> 5030000432
	5030000576 [label=ConvolutionBackward0]
	5030000768 -> 5030000576
	5030000768 [label=ReluBackward0]
	5030000912 -> 5030000768
	5030000912 [label=NativeBatchNormBackward0]
	5030001008 -> 5030000912
	5030001008 [label=ConvolutionBackward0]
	5030001200 -> 5030001008
	5030001200 [label=ReluBackward0]
	5030001344 -> 5030001200
	5030001344 [label=NativeBatchNormBackward0]
	5030001440 -> 5030001344
	5030001440 [label=ConvolutionBackward0]
	5030001632 -> 5030001440
	5030001632 [label=MaxPool2DWithIndicesBackward0]
	5030001776 -> 5030001632
	5030001776 [label=ReluBackward0]
	5030001872 -> 5030001776
	5030001872 [label=NativeBatchNormBackward0]
	5030001968 -> 5030001872
	5030001968 [label=ConvolutionBackward0]
	5030002160 -> 5030001968
	5029684992 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	5029684992 -> 5030002160
	5030002160 [label=AccumulateGrad]
	5030001920 -> 5030001872
	5029598272 [label="bn1.weight
 (64)" fillcolor=lightblue]
	5029598272 -> 5030001920
	5030001920 [label=AccumulateGrad]
	5030001680 -> 5030001872
	4961237600 [label="bn1.bias
 (64)" fillcolor=lightblue]
	4961237600 -> 5030001680
	5030001680 [label=AccumulateGrad]
	5030001584 -> 5030001440
	5028878176 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	5028878176 -> 5030001584
	5030001584 [label=AccumulateGrad]
	5030001392 -> 5030001344
	5028877296 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	5028877296 -> 5030001392
	5030001392 [label=AccumulateGrad]
	5030001248 -> 5030001344
	5028875056 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	5028875056 -> 5030001248
	5030001248 [label=AccumulateGrad]
	5030001152 -> 5030001008
	5028880496 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	5028880496 -> 5030001152
	5030001152 [label=AccumulateGrad]
	5030000960 -> 5030000912
	5028878016 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	5028878016 -> 5030000960
	5030000960 [label=AccumulateGrad]
	5030000816 -> 5030000912
	5028881616 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	5028881616 -> 5030000816
	5030000816 [label=AccumulateGrad]
	5030000720 -> 5030000576
	4961664624 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	4961664624 -> 5030000720
	5030000720 [label=AccumulateGrad]
	5030000528 -> 5030000432
	4961654064 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	4961654064 -> 5030000528
	5030000528 [label=AccumulateGrad]
	5030000480 -> 5030000432
	4961659264 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	4961659264 -> 5030000480
	5030000480 [label=AccumulateGrad]
	5030000384 -> 5030000336
	5030000384 [label=NativeBatchNormBackward0]
	5030001104 -> 5030000384
	5030001104 [label=ConvolutionBackward0]
	5030001632 -> 5030001104
	5030001488 -> 5030001104
	5029686032 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	5029686032 -> 5030001488
	5030001488 [label=AccumulateGrad]
	5030000672 -> 5030000384
	5196345520 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	5196345520 -> 5030000672
	5030000672 [label=AccumulateGrad]
	5030000624 -> 5030000384
	5196345840 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	5196345840 -> 5030000624
	5030000624 [label=AccumulateGrad]
	5030000240 -> 5030000048
	4941792576 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	4941792576 -> 5030000240
	5030000240 [label=AccumulateGrad]
	5030000000 -> 5029999952
	4941802896 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	4941802896 -> 5030000000
	5030000000 [label=AccumulateGrad]
	5029999856 -> 5029999952
	4941802816 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	4941802816 -> 5029999856
	5029999856 [label=AccumulateGrad]
	5029999760 -> 5029999616
	4941801296 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	4941801296 -> 5029999760
	5029999760 [label=AccumulateGrad]
	5029999568 -> 5029999520
	4941801376 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	4941801376 -> 5029999568
	5029999568 [label=AccumulateGrad]
	5029999424 -> 5029999520
	4941801776 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	4941801776 -> 5029999424
	5029999424 [label=AccumulateGrad]
	5029999328 -> 5029999184
	4961233680 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	4961233680 -> 5029999328
	5029999328 [label=AccumulateGrad]
	5029999136 -> 5029999040
	4961233600 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	4961233600 -> 5029999136
	5029999136 [label=AccumulateGrad]
	5029999088 -> 5029999040
	4961233520 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	4961233520 -> 5029999088
	5029999088 [label=AccumulateGrad]
	5029998992 -> 5029998944
	5029998848 -> 5029998656
	4961232160 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	4961232160 -> 5029998848
	5029998848 [label=AccumulateGrad]
	5029998608 -> 5029998560
	4961232480 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	4961232480 -> 5029998608
	5029998608 [label=AccumulateGrad]
	5029998464 -> 5029998560
	4961232400 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	4961232400 -> 5029998464
	5029998464 [label=AccumulateGrad]
	5029998368 -> 5029998224
	4961231840 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	4961231840 -> 5029998368
	5029998368 [label=AccumulateGrad]
	5029998176 -> 5029998128
	4961231520 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	4961231520 -> 5029998176
	5029998176 [label=AccumulateGrad]
	5029998032 -> 5029998128
	4961231760 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	4961231760 -> 5029998032
	5029998032 [label=AccumulateGrad]
	5029997936 -> 5029997792
	4961230560 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	4961230560 -> 5029997936
	5029997936 [label=AccumulateGrad]
	5029997744 -> 5029997648
	4961230480 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	4961230480 -> 5029997744
	5029997744 [label=AccumulateGrad]
	5029997696 -> 5029997648
	4961230400 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	4961230400 -> 5029997696
	5029997696 [label=AccumulateGrad]
	5029997600 -> 5029997552
	5029997360 -> 5029995632
	4880010672 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	4880010672 -> 5029997360
	5029997360 [label=AccumulateGrad]
	5029995680 -> 5029995776
	4880011872 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	4880011872 -> 5029995680
	5029995680 [label=AccumulateGrad]
	5029995872 -> 5029995776
	4880009632 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	4880009632 -> 5029995872
	5029995872 [label=AccumulateGrad]
	5029996064 -> 5029996208
	5094636608 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	5094636608 -> 5029996064
	5029996064 [label=AccumulateGrad]
	5029996256 -> 5029996304
	5094637168 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	5094637168 -> 5029996256
	5029996256 [label=AccumulateGrad]
	5029996400 -> 5029996304
	5094637328 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	5094637328 -> 5029996400
	5029996400 [label=AccumulateGrad]
	5029996640 -> 5029996784
	5094312032 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	5094312032 -> 5029996640
	5029996640 [label=AccumulateGrad]
	5029996832 -> 5029996928
	5094312272 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	5094312272 -> 5029996832
	5029996832 [label=AccumulateGrad]
	5029996880 -> 5029996928
	5094313952 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	5094313952 -> 5029996880
	5029996880 [label=AccumulateGrad]
	5029996976 -> 5029997024
	5029996976 [label=NativeBatchNormBackward0]
	5029996112 -> 5029996976
	5029996112 [label=ConvolutionBackward0]
	5029997408 -> 5029996112
	5029995536 -> 5029996112
	4961229680 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	4961229680 -> 5029995536
	5029995536 [label=AccumulateGrad]
	5029996688 -> 5029996976
	4961229360 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	4961229360 -> 5029996688
	5029996688 [label=AccumulateGrad]
	5029996736 -> 5029996976
	4961229280 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	4961229280 -> 5029996736
	5029996736 [label=AccumulateGrad]
	5029997120 -> 5029993184
	5094310032 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	5094310032 -> 5029997120
	5029997120 [label=AccumulateGrad]
	5029993040 -> 5029996544
	5094310112 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	5094310112 -> 5029993040
	5029993040 [label=AccumulateGrad]
	5029997312 -> 5029996544
	5094310272 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	5094310272 -> 5029997312
	5029997312 [label=AccumulateGrad]
	5029988576 -> 5029986608
	5094310832 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	5094310832 -> 5029988576
	5029988576 [label=AccumulateGrad]
	5029986512 -> 5029986656
	5094310672 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	5094310672 -> 5029986512
	5029986512 [label=AccumulateGrad]
	5029988480 -> 5029986656
	5094310992 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	5094310992 -> 5029988480
	5029988480 [label=AccumulateGrad]
	5029988192 -> 5029988336
	5094319072 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	5094319072 -> 5029988192
	5029988192 [label=AccumulateGrad]
	5029987616 -> 5029987712
	5094320352 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	5094320352 -> 5029987616
	5029987616 [label=AccumulateGrad]
	5029987664 -> 5029987712
	5094320992 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	5094320992 -> 5029987664
	5029987664 [label=AccumulateGrad]
	5029987952 -> 5029987808
	5029987760 -> 5029988096
	5094323472 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	5094323472 -> 5029987760
	5029987760 [label=AccumulateGrad]
	5029988384 -> 5029987520
	5094324112 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	5094324112 -> 5029988384
	5029988384 [label=AccumulateGrad]
	5029986992 -> 5029987520
	5094324752 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	5094324752 -> 5029986992
	5029986992 [label=AccumulateGrad]
	5029986896 -> 5029987040
	5196368208 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	5196368208 -> 5029986896
	5029986896 [label=AccumulateGrad]
	5029987088 -> 5029986752
	5196366288 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	5196366288 -> 5029987088
	5029987088 [label=AccumulateGrad]
	5029987280 -> 5029986752
	5196368848 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	5196368848 -> 5029987280
	5029987280 [label=AccumulateGrad]
	5029987232 -> 5029987472
	5196371968 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	5196371968 -> 5029987232
	5029987232 [label=AccumulateGrad]
	5029987568 -> 5029988528
	5196372608 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	5196372608 -> 5029987568
	5029987568 [label=AccumulateGrad]
	5029988432 -> 5029988528
	5196373248 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	5196373248 -> 5029988432
	5029988432 [label=AccumulateGrad]
	5029988672 -> 5029992176
	5029992272 -> 5029991840
	5196375168 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	5196375168 -> 5029992272
	5029992272 [label=AccumulateGrad]
	5029991648 -> 5029991600
	5196375808 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	5196375808 -> 5029991648
	5029991648 [label=AccumulateGrad]
	5029991552 -> 5029991600
	5196376448 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	5196376448 -> 5029991552
	5029991552 [label=AccumulateGrad]
	5029991408 -> 5029991456
	4882762624 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	4882762624 -> 5029991408
	5029991408 [label=AccumulateGrad]
	5029991216 -> 5029991120
	4882762704 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	4882762704 -> 5029991216
	5029991216 [label=AccumulateGrad]
	5029991168 -> 5029991120
	4882762544 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	4882762544 -> 5029991168
	5029991168 [label=AccumulateGrad]
	5029990880 -> 5029990784
	4882766704 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	4882766704 -> 5029990880
	5029990880 [label=AccumulateGrad]
	5029990976 -> 5029990544
	4882764144 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	4882764144 -> 5029990976
	5029990976 [label=AccumulateGrad]
	5029990688 -> 5029990544
	4882764544 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	4882764544 -> 5029990688
	5029990688 [label=AccumulateGrad]
	5029990496 -> 5029990592
	5029990352 -> 5029991504
	4882765264 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	4882765264 -> 5029990352
	5029990352 [label=AccumulateGrad]
	5029990208 -> 5029990112
	4882763824 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	4882763824 -> 5029990208
	5029990208 [label=AccumulateGrad]
	5029990256 -> 5029990112
	4882763504 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	4882763504 -> 5029990256
	5029990256 [label=AccumulateGrad]
	5029989920 -> 5029989968
	5078582752 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	5078582752 -> 5029989920
	5029989920 [label=AccumulateGrad]
	5029990064 -> 5029989392
	4485212384 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	4485212384 -> 5029990064
	5029990064 [label=AccumulateGrad]
	5029989440 -> 5029989392
	5029681952 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	5029681952 -> 5029989440
	5029989440 [label=AccumulateGrad]
	5029989536 -> 5029989680
	4941800256 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	4941800256 -> 5029989536
	5029989536 [label=AccumulateGrad]
	5029989728 -> 5029989152
	4941800176 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	4941800176 -> 5029989728
	5029989728 [label=AccumulateGrad]
	5029989248 -> 5029989152
	4941800096 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	4941800096 -> 5029989248
	5029989248 [label=AccumulateGrad]
	5029989104 -> 5029989200
	5029989104 [label=NativeBatchNormBackward0]
	5029989824 -> 5029989104
	5029989824 [label=ConvolutionBackward0]
	5029990736 -> 5029989824
	5029991024 -> 5029989824
	4882765744 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	4882765744 -> 5029991024
	5029991024 [label=AccumulateGrad]
	5029989632 -> 5029989104
	4882765104 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	4882765104 -> 5029989632
	5029989632 [label=AccumulateGrad]
	5029989488 -> 5029989104
	4882761344 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	4882761344 -> 5029989488
	5029989488 [label=AccumulateGrad]
	5029989008 -> 5029991792
	4941802176 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	4941802176 -> 5029989008
	5029989008 [label=AccumulateGrad]
	5029991936 -> 5029992032
	4941802336 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	4941802336 -> 5029991936
	5029991936 [label=AccumulateGrad]
	5029992080 -> 5029992032
	4941799776 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	4941799776 -> 5029992080
	5029992080 [label=AccumulateGrad]
	5029988816 -> 5029992128
	4941801536 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	4941801536 -> 5029988816
	5029988816 [label=AccumulateGrad]
	5029992320 -> 5029992368
	4941797936 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	4941797936 -> 5029992320
	5029992320 [label=AccumulateGrad]
	5029992512 -> 5029992368
	4941801456 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	4941801456 -> 5029992512
	5029992512 [label=AccumulateGrad]
	5029992560 -> 5029992416
	4941792416 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	4941792416 -> 5029992560
	5029992560 [label=AccumulateGrad]
	5029992608 -> 5029992704
	4941792256 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	4941792256 -> 5029992608
	5029992608 [label=AccumulateGrad]
	5029992752 -> 5029992704
	4941792016 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	4941792016 -> 5029992752
	5029992752 [label=AccumulateGrad]
	5029992800 -> 5029992656
	5029992992 -> 5029993616
	4941798176 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	4941798176 -> 5029992992
	5029992992 [label=AccumulateGrad]
	5029993712 -> 5029993808
	4941805216 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	4941805216 -> 5029993712
	5029993712 [label=AccumulateGrad]
	5029994960 -> 5029993808
	4941805136 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	4941805136 -> 5029994960
	5029994960 [label=AccumulateGrad]
	5029994624 -> 5029994768
	4941791376 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	4941791376 -> 5029994624
	5029994624 [label=AccumulateGrad]
	5029994912 -> 5029994864
	4941796416 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	4941796416 -> 5029994912
	5029994912 [label=AccumulateGrad]
	5029994576 -> 5029994864
	4941795136 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	4941795136 -> 5029994576
	5029994576 [label=AccumulateGrad]
	5029994480 -> 5029996448
	4941794816 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	4941794816 -> 5029994480
	5029994480 [label=AccumulateGrad]
	5029993520 -> 5029993232
	4941795776 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	4941795776 -> 5029993520
	5029993520 [label=AccumulateGrad]
	5029993280 -> 5029993232
	4941794976 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	4941794976 -> 5029993280
	5029993280 [label=AccumulateGrad]
	5029993376 -> 5029993472
	5029993088 -> 5029994000
	4941796496 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	4941796496 -> 5029993088
	5029993088 [label=AccumulateGrad]
	5029994336 -> 5029994288
	4941795376 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	4941795376 -> 5029994336
	5029994336 [label=AccumulateGrad]
	5029994192 -> 5029994288
	4941803696 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	4941803696 -> 5029994192
	5029994192 [label=AccumulateGrad]
	5029994144 -> 5028706000
	4941797696 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	4941797696 -> 5029994144
	5029994144 [label=AccumulateGrad]
	5028706144 -> 5028707008
	4941798096 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	4941798096 -> 5028706144
	5028706144 [label=AccumulateGrad]
	5028706096 -> 5028707008
	4941803376 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	4941803376 -> 5028706096
	5028706096 [label=AccumulateGrad]
	5028707056 -> 5028707200
	4941801136 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	4941801136 -> 5028707056
	5028707056 [label=AccumulateGrad]
	5028708064 -> 5028707248
	4941802496 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	4941802496 -> 5028708064
	5028708064 [label=AccumulateGrad]
	5028707680 -> 5028707248
	4941803136 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	4941803136 -> 5028707680
	5028707680 [label=AccumulateGrad]
	5028707152 -> 5028705856
	5028704128 -> 5028703696
	5030144464 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	5030144464 -> 5028704128
	5028704128 [label=AccumulateGrad]
	5028703888 -> 5028703840
	5030133904 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	5030133904 -> 5028703888
	5028703888 [label=AccumulateGrad]
	5028703984 -> 5028703840
	5030141984 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	5030141984 -> 5028703984
	5028703984 [label=AccumulateGrad]
	5028703744 -> 5028696640
	5030140784 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	5030140784 -> 5028703744
	5028703744 [label=AccumulateGrad]
	5028703600 -> 5028703552
	5030143744 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	5030143744 -> 5028703600
	5028703600 [label=AccumulateGrad]
	5028699328 -> 5028703552
	5030148224 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	5030148224 -> 5028699328
	5028699328 [label=AccumulateGrad]
	5028699376 -> 5028699088
	5030141264 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	5030141264 -> 5028699376
	5028699376 [label=AccumulateGrad]
	5028698992 -> 5028699136
	5030138064 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	5030138064 -> 5028698992
	5028698992 [label=AccumulateGrad]
	5028696112 -> 5028699136
	5030134144 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	5030134144 -> 5028696112
	5028696112 [label=AccumulateGrad]
	5028696160 -> 5028696208
	5028700144 -> 5028700336
	5030136464 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	5030136464 -> 5028700144
	5028700144 [label=AccumulateGrad]
	5028700240 -> 5028700096
	5030150064 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	5030150064 -> 5028700240
	5028700240 [label=AccumulateGrad]
	5028699904 -> 5028700096
	5030149984 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	5030149984 -> 5028699904
	5028699904 [label=AccumulateGrad]
	5028700000 -> 5028699760
	5030149024 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	5030149024 -> 5028700000
	5028700000 [label=AccumulateGrad]
	5028699568 -> 5028698080
	5030149104 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	5030149104 -> 5028699568
	5028699568 [label=AccumulateGrad]
	5028704704 -> 5028698080
	5030148864 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	5030148864 -> 5028704704
	5028704704 [label=AccumulateGrad]
	5028705328 -> 5028705088
	5030148304 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	5030148304 -> 5028705328
	5028705328 [label=AccumulateGrad]
	5028703408 -> 5028704320
	5030148144 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	5030148144 -> 5028703408
	5028703408 [label=AccumulateGrad]
	5028703456 -> 5028704320
	5030148064 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	5030148064 -> 5028703456
	5028703456 [label=AccumulateGrad]
	5028704416 -> 5028704848
	5028699520 -> 5028706576
	5030146464 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	5030146464 -> 5028699520
	5028699520 [label=AccumulateGrad]
	5028706672 -> 5028706624
	5030146224 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	5030146224 -> 5028706672
	5028706672 [label=AccumulateGrad]
	5028705616 -> 5028706624
	5030146144 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	5030146144 -> 5028705616
	5028705616 [label=AccumulateGrad]
	5028705424 -> 5028701728
	5030145264 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	5030145264 -> 5028705424
	5028705424 [label=AccumulateGrad]
	5028701632 -> 5028701680
	5030145424 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	5030145424 -> 5028701632
	5028701632 [label=AccumulateGrad]
	5028701008 -> 5028701680
	5030145184 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	5030145184 -> 5028701008
	5028701008 [label=AccumulateGrad]
	5028700960 -> 5028701152
	5030144384 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	5030144384 -> 5028700960
	5028700960 [label=AccumulateGrad]
	5028701200 -> 5028701536
	5030144224 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	5030144224 -> 5028701200
	5028701200 [label=AccumulateGrad]
	5028701296 -> 5028701536
	5030144144 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	5030144144 -> 5028701296
	5028701296 [label=AccumulateGrad]
	5028701440 -> 5028701392
	5028701440 [label=NativeBatchNormBackward0]
	5028705472 -> 5028701440
	5028705472 [label=ConvolutionBackward0]
	5028704464 -> 5028705472
	5028696256 -> 5028705472
	5030147424 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	5030147424 -> 5028696256
	5028696256 [label=AccumulateGrad]
	5028701344 -> 5028701440
	5030147344 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	5030147344 -> 5028701344
	5028701344 [label=AccumulateGrad]
	5028701248 -> 5028701440
	5030147184 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	5030147184 -> 5028701248
	5028701248 [label=AccumulateGrad]
	5028700480 -> 5028698608
	5030143584 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	5030143584 -> 5028700480
	5028700480 [label=AccumulateGrad]
	5028698704 -> 5028698416
	5030143664 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	5030143664 -> 5028698704
	5028698704 [label=AccumulateGrad]
	5028697792 -> 5028698416
	5030143264 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	5030143264 -> 5028697792
	5028697792 [label=AccumulateGrad]
	5028697504 -> 5028697408
	5030142384 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	5030142384 -> 5028697504
	5028697504 [label=AccumulateGrad]
	5028697360 -> 5028697264
	5030142624 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	5030142624 -> 5028697360
	5028697360 [label=AccumulateGrad]
	5028697312 -> 5028697264
	5030142304 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	5030142304 -> 5028697312
	5028697312 [label=AccumulateGrad]
	5028697696 -> 5028702112
	5030141504 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	5030141504 -> 5028697696
	5028697696 [label=AccumulateGrad]
	5028702016 -> 5028702064
	5030141424 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	5030141424 -> 5028702016
	5028702016 [label=AccumulateGrad]
	5028701968 -> 5028702064
	5030141184 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	5030141184 -> 5028701968
	5028701968 [label=AccumulateGrad]
	5028703216 -> 5028702928
	5028702784 -> 5028702736
	5030140304 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	5030140304 -> 5028702784
	5028702784 [label=AccumulateGrad]
	5028702832 -> 5028702544
	5030140224 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	5030140224 -> 5028702832
	5028702832 [label=AccumulateGrad]
	5028699712 -> 5028702544
	5030140064 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	5030140064 -> 5028699712
	5028699712 [label=AccumulateGrad]
	5028704368 -> 5028698224
	5030139104 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	5030139104 -> 5028704368
	5028704368 [label=AccumulateGrad]
	5028698320 -> 5028698368
	5030139344 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	5030139344 -> 5028698320
	5028698320 [label=AccumulateGrad]
	5028697072 -> 5028698368
	5030139024 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	5030139024 -> 5028697072
	5028697072 [label=AccumulateGrad]
	5028697120 -> 5028697648
	5030138144 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	5030138144 -> 5028697120
	5028697120 [label=AccumulateGrad]
	5028700864 -> 5028700720
	5030137984 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	5030137984 -> 5028700864
	5028700864 [label=AccumulateGrad]
	5028701056 -> 5028700720
	5030137904 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	5030137904 -> 5028701056
	5028701056 [label=AccumulateGrad]
	5028700816 -> 5028701104
	5028706816 -> 5028707296
	5028706816 [label=TBackward0]
	5028699472 -> 5028706816
	5196447664 [label="fc.weight
 (14, 2048)" fillcolor=lightblue]
	5196447664 -> 5028699472
	5028699472 [label=AccumulateGrad]
	5028707296 -> 5029688032
}
