{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwzqNd1Vg4wi"
      },
      "source": [
        "## Load the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wtdbnguQ4aY"
      },
      "source": [
        "Mount Google Drive to access data and other repo files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjKkw5H68gM0",
        "outputId": "48137c92-8999-4950-8e38-0319a89ee6b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bfgRV3ZQ8tX"
      },
      "source": [
        "Set the seed for reproducability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JD7I8qKXQ38W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUjGGMxp_4H_"
      },
      "source": [
        "Delete redundant sample data from Google Colab's session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g46yhAoyLeuB",
        "outputId": "a6d5826f-af36-446f-83d6-b0df54670723"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Folder 'sample_data' has been deleted.\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "def delete_folder(folder_path):\n",
        "    if os.path.exists(folder_path):\n",
        "        shutil.rmtree(folder_path)\n",
        "        print(f\"Folder '{folder_path}' has been deleted.\")\n",
        "    else:\n",
        "        print(f\"Folder '{folder_path}' does not exist.\")\n",
        "\n",
        "delete_folder('sample_data')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v77OIPU_4IB"
      },
      "source": [
        "Clone the repository to access the other relevant files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezFpCBHn9-Xl",
        "outputId": "00640f18-3046-49f6-b1f5-45af68b0bc8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Deep-Self-Learning-From-Noisy-Labels'...\n",
            "remote: Enumerating objects: 164, done.\u001b[K\n",
            "remote: Counting objects: 100% (164/164), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "remote: Total 164 (delta 91), reused 127 (delta 58), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (164/164), 4.22 MiB | 8.56 MiB/s, done.\n",
            "Resolving deltas: 100% (91/91), done.\n",
            "/content/Deep-Self-Learning-From-Noisy-Labels\n"
          ]
        }
      ],
      "source": [
        "# clone the repo\n",
        "!git clone https://github.com/lgiesen/Deep-Self-Learning-From-Noisy-Labels.git\n",
        "\n",
        "# go to directory\n",
        "%cd Deep-Self-Learning-From-Noisy-Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyXJNDnV_4IE"
      },
      "source": [
        "Define the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJGD8fw88gM5",
        "outputId": "575ed3d1-9885-49e5-8184-94ee8ee84a5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train': 674373, 'val': 207499}\n",
            "/\n"
          ]
        }
      ],
      "source": [
        "from config import batch_size, dataset_test_path, dataset_train_path, dataset_val_path\n",
        "from LoadDataset import CustomImageDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# Define the transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.RandomCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    # \"These exact values are used for normalizing data that has been pre-trained\n",
        "    # on the ImageNet dataset. They are based on the statistics of the ImageNet\n",
        "    # dataset, which consists of a large number of natural images.\"\n",
        "    # https://moiseevigor.github.io/software/2022/12/18/one-pager-training-resnet-on-imagenet/\n",
        "\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = CustomImageDataset(file_path=dataset_train_path, transform=transform)\n",
        "val_dataset = CustomImageDataset(file_path=dataset_val_path, transform=transform)\n",
        "test_dataset = CustomImageDataset(file_path=dataset_test_path, transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "# pinned memory can significantly speed up the transfer of data between the host and the device (GPU) because the GPU can directly access it\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Prepare dataloaders dictionary\n",
        "dataloaders = {\n",
        "    'train': train_loader,\n",
        "    'val': val_loader\n",
        "}\n",
        "\n",
        "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
        "print(dataset_sizes)\n",
        "%cd ../../"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzDPbook_4IF"
      },
      "source": [
        "Extract the image files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGoMbLEw20XY",
        "outputId": "47a07ad3-253b-4efc-ee35-ad1401f0aa44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted /content/drive/MyDrive/Colab_Notebooks/Deep_Self_Learning_From_Noisy_Labels/images/0.tar to ../drive/MyDrive/Colab_Notebooks/Deep_Self_Learning_From_Noisy_Labels/extracted_images/\n",
            "Extracted /content/drive/MyDrive/Colab_Notebooks/Deep_Self_Learning_From_Noisy_Labels/images/5.tar to ../drive/MyDrive/Colab_Notebooks/Deep_Self_Learning_From_Noisy_Labels/extracted_images/\n",
            "Extracted /content/drive/MyDrive/Colab_Notebooks/Deep_Self_Learning_From_Noisy_Labels/images/9.tar to ../drive/MyDrive/Colab_Notebooks/Deep_Self_Learning_From_Noisy_Labels/extracted_images/\n",
            "Extracted /content/drive/MyDrive/Colab_Notebooks/Deep_Self_Learning_From_Noisy_Labels/images/4.tar to ../drive/MyDrive/Colab_Notebooks/Deep_Self_Learning_From_Noisy_Labels/extracted_images/\n",
            "Extracted /content/drive/MyDrive/Colab_Notebooks/Deep_Self_Learning_From_Noisy_Labels/images/2.tar to ../drive/MyDrive/Colab_Notebooks/Deep_Self_Learning_From_Noisy_Labels/extracted_images/\n",
            "Extracted /content/drive/MyDrive/Colab_Notebooks/Deep_Self_Learning_From_Noisy_Labels/images/7.tar to ../drive/MyDrive/Colab_Notebooks/Deep_Self_Learning_From_Noisy_Labels/extracted_images/\n",
            "Extracted /content/drive/MyDrive/Colab_Notebooks/Deep_Self_Learning_From_Noisy_Labels/images/3.tar to ../drive/MyDrive/Colab_Notebooks/Deep_Self_Learning_From_Noisy_Labels/extracted_images/\n",
            "Extracted /content/drive/MyDrive/Colab_Notebooks/Deep_Self_Learning_From_Noisy_Labels/images/1.tar to ../drive/MyDrive/Colab_Notebooks/Deep_Self_Learning_From_Noisy_Labels/extracted_images/\n",
            "Extracted /content/drive/MyDrive/Colab_Notebooks/Deep_Self_Learning_From_Noisy_Labels/images/8.tar to ../drive/MyDrive/Colab_Notebooks/Deep_Self_Learning_From_Noisy_Labels/extracted_images/\n",
            "Extracted /content/drive/MyDrive/Colab_Notebooks/Deep_Self_Learning_From_Noisy_Labels/images/6.tar to ../drive/MyDrive/Colab_Notebooks/Deep_Self_Learning_From_Noisy_Labels/extracted_images/\n",
            "The extracted tar files should result in the folders 0 to 9:\n",
            "0  1  2  3  4  5  6  7\t8  9\n",
            "CPU times: user 3min 46s, sys: 6min 13s, total: 10min\n",
            "Wall time: 6min 6s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import tarfile\n",
        "import os\n",
        "from config import shared_folder_path, dataset_img\n",
        "\n",
        "# Function to extract and process files\n",
        "def extract_and_process(tar_file_path, extract_to):\n",
        "    with tarfile.open(tar_file_path, 'r') as tar_ref:\n",
        "        tar_ref.extractall(extract_to)\n",
        "        print(f\"Extracted {tar_file_path} to {extract_to}\")\n",
        "\n",
        "parallel_extraction = True\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Function to extract and process files\n",
        "def extract_and_process(tar_file_path, extract_to):\n",
        "    with tarfile.open(tar_file_path, 'r') as tar_ref:\n",
        "        tar_ref.extractall(extract_to)\n",
        "        print(f\"Extracted {tar_file_path} to {extract_to}\")\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(dataset_img, exist_ok=True)\n",
        "\n",
        "# List of tar files to extract\n",
        "tar_files = [os.path.join(shared_folder_path, f\"{i}.tar\") for i in range(10)]\n",
        "\n",
        "# Function to handle extraction in parallel\n",
        "def extract_tar_files_parallel(tar_files, extract_to):\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        futures = [executor.submit(extract_and_process, tar_file, extract_to) for tar_file in tar_files if os.path.exists(tar_file)]\n",
        "        for future in futures:\n",
        "            try:\n",
        "                future.result()  # Wait for the result to ensure any exceptions are raised\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Extract tar files in parallel\n",
        "extract_tar_files_parallel(tar_files, dataset_img)\n",
        "print(\"The extracted tar files should result in the folders 0 to 9:\")\n",
        "!ls \"{dataset_img}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kg3-V9AjhANX"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5WIYVsj_4IG",
        "outputId": "ec460f8f-e360-41f3-b229-68e8ef03acce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cuda:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:01<00:00, 74.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "from config import lr, momentum, weight_decay, gamma, step_size, dataset, num_classes\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from collections import Counter\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device: {device}\")\n",
        "\n",
        "# Initialize the model\n",
        "#model = models.resnet50(pretrained=True)\n",
        "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "# Modify the final fully connected layer to output 14 classes\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(num_ftrs, num_classes)\n",
        "# Parallelize training across multiple GPUs\n",
        "model = torch.nn.DataParallel(model)\n",
        "\n",
        "# Calculate the balanced class weights because of an imbalanced dataset\n",
        "# Read the data again for higher efficiency\n",
        "data = pd.read_csv(dataset.replace(\"../\",\"/content/\"), header=None, sep=' ', usecols=[1], names=['label'])\n",
        "# Convert the labels to a numpy array\n",
        "labels = data['label'].values\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "del labels, data\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "# Initialize the learning rate scheduler: Decay LR by a factor of 0.1 every 5 epochs\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HgEIt5V4gJJi"
      },
      "outputs": [],
      "source": [
        "# Function to calculate accuracy\n",
        "def calculate_accuracy(outputs, labels):\n",
        "    _, predicted = outputs.max(1)\n",
        "    correct = predicted.eq(labels).sum().item()\n",
        "    return correct\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(loader, model, criterion, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():  # No need to calculate gradients during evaluation\n",
        "        for inputs, labels in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            correct_predictions += calculate_accuracy(outputs, labels)\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    avg_loss = running_loss / len(loader)\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2JgD78Wf_4IH",
        "outputId": "423cd7a6-8f3c-4ccf-a47e-b56af214f3ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15, Train Loss: 0.8193, Train Accuracy: 0.7395, Val Loss: 0.6566, Val Accuracy: 0.7911, Time: 2209.52 sec\n",
            "Epoch 2/15, Train Loss: 0.6084, Train Accuracy: 0.8045, Val Loss: 0.6293, Val Accuracy: 0.7987, Time: 1997.41 sec\n",
            "Epoch 3/15, Train Loss: 0.6120, Train Accuracy: 0.8038, Val Loss: 0.6734, Val Accuracy: 0.7913, Time: 1954.85 sec\n",
            "Epoch 4/15, Train Loss: 0.6619, Train Accuracy: 0.7893, Val Loss: 0.8744, Val Accuracy: 0.7246, Time: 1975.22 sec\n",
            "Epoch 5/15, Train Loss: 0.7066, Train Accuracy: 0.7775, Val Loss: 0.8256, Val Accuracy: 0.7476, Time: 1967.66 sec\n",
            "Epoch 6/15, Train Loss: 0.5371, Train Accuracy: 0.8306, Val Loss: 0.5273, Val Accuracy: 0.8344, Time: 1982.76 sec\n",
            "Epoch 7/15, Train Loss: 0.4845, Train Accuracy: 0.8473, Val Loss: 0.5134, Val Accuracy: 0.8348, Time: 1970.28 sec\n",
            "Epoch 8/15, Train Loss: 0.4581, Train Accuracy: 0.8560, Val Loss: 0.5120, Val Accuracy: 0.8388, Time: 1973.72 sec\n",
            "Epoch 9/15, Train Loss: 0.4409, Train Accuracy: 0.8621, Val Loss: 0.5125, Val Accuracy: 0.8419, Time: 1968.09 sec\n",
            "Epoch 10/15, Train Loss: 0.4271, Train Accuracy: 0.8668, Val Loss: 0.5194, Val Accuracy: 0.8424, Time: 1970.11 sec\n",
            "Epoch 11/15, Train Loss: 0.3610, Train Accuracy: 0.8891, Val Loss: 0.4814, Val Accuracy: 0.8552, Time: 1979.23 sec\n",
            "Epoch 12/15, Train Loss: 0.3391, Train Accuracy: 0.8966, Val Loss: 0.4815, Val Accuracy: 0.8540, Time: 1978.22 sec\n",
            "Epoch 13/15, Train Loss: 0.3276, Train Accuracy: 0.9005, Val Loss: 0.4823, Val Accuracy: 0.8555, Time: 1974.32 sec\n",
            "Epoch 14/15, Train Loss: 0.3174, Train Accuracy: 0.9037, Val Loss: 0.4837, Val Accuracy: 0.8566, Time: 1976.63 sec\n",
            "Epoch 15/15, Train Loss: 0.3086, Train Accuracy: 0.9067, Val Loss: 0.4876, Val Accuracy: 0.8579, Time: 1981.78 sec\n",
            "Finished Training, Final Train Loss: 0.3086, Final Train Accuracy: 0.9067\n",
            "Test Loss: 0.4850, Test Accuracy: 0.8584\n",
            "Test Loss: 0.4850, Test Accuracy: 0.8584\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Parent directory ../drive/MyDrive/Colab_Notebooks/Deep_Self_Learning_From_Noisy_Labels/models does not exist.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-fb72327b2d03>\u001b[0m in \u001b[0;36m<cell line: 95>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{dataset_root}models/resnet50_clothing1m.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model saved.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_disable_byteorder_record\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Parent directory ../drive/MyDrive/Colab_Notebooks/Deep_Self_Learning_From_Noisy_Labels/models does not exist."
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Parent directory ../drive/MyDrive/Colab_Notebooks/Deep_Self_Learning_From_Noisy_Labels/models does not exist.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-fb72327b2d03>\u001b[0m in \u001b[0;36m<cell line: 95>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{dataset_root}models/resnet50_clothing1m.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model saved.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_disable_byteorder_record\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Parent directory ../drive/MyDrive/Colab_Notebooks/Deep_Self_Learning_From_Noisy_Labels/models does not exist."
          ]
        }
      ],
      "source": [
        "import time\n",
        "from config import num_epochs, dataset_root, checkpoint_path\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Initialize TensorBoard writer\n",
        "writer_path = dataset_root.replace(\"..\", \"/content\") + 'runs/resnet50_experiment'\n",
        "writer = SummaryWriter(writer_path)\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "early_stop = False\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    if early_stop:\n",
        "        break\n",
        "\n",
        "    model.train()  # Set model to training mode\n",
        "    epoch_start_time = time.time()  # Start time for the epoch\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        # Move input and label tensors to the device\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Zero out the optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        correct_predictions += calculate_accuracy(outputs, labels)\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    epoch_duration = time.time() - epoch_start_time  # End time for the epoch\n",
        "    avg_loss = running_loss / len(train_loader)  # Average loss for the epoch\n",
        "    accuracy = correct_predictions / total_samples  # Accuracy for the epoch\n",
        "\n",
        "    # Log the training loss, accuracy, and duration to TensorBoard\n",
        "    writer.add_scalar('Loss/train', avg_loss, epoch)\n",
        "    writer.add_scalar('Accuracy/train', accuracy, epoch)\n",
        "    writer.add_scalar('Time/train', epoch_duration, epoch)\n",
        "\n",
        "    # Validate the model\n",
        "    val_loss, val_accuracy = evaluate_model(val_loader, model, criterion, device)\n",
        "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
        "    writer.add_scalar('Accuracy/val', val_accuracy, epoch)\n",
        "\n",
        "    # Print the loss, accuracy, and time for every epoch\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "          f'Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.4f}, '\n",
        "          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, '\n",
        "          f'Time: {epoch_duration:.2f} sec')\n",
        "\n",
        "    # Check for early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f'Early stopping at epoch {epoch+1}')\n",
        "            early_stop = True\n",
        "\n",
        "    # Save checkpoint every epoch\n",
        "    torch.save(model.state_dict(), checkpoint_path + f'model_epoch_{epoch+1}.pth')\n",
        "\n",
        "    # Step the scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "print(f'Finished Training, Final Train Loss: {avg_loss:.4f}, Final Train Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Test the model\n",
        "test_loss, test_accuracy = evaluate_model(test_loader, model, criterion, device)\n",
        "writer.add_scalar('Loss/test', test_loss, 0)\n",
        "writer.add_scalar('Accuracy/test', test_accuracy, 0)\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "# Close the TensorBoard writer\n",
        "writer.close()\n",
        "\n",
        "model_path = f'{dataset_root}models/resnet50_clothing1m.pth'\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(\"Model saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HV6Si1ZuasqX",
        "outputId": "997ca288-929a-4e93-d3b9-c6b5b9159086"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved.\n"
          ]
        }
      ],
      "source": [
        "from config import model_path_standard as model_path\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(\"Model saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "iz2VNCT8a3-P",
        "outputId": "9938243e-36b6-4221-8b12-9e455593b0ca"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_4737e04b-3064-4851-bfb6-eabfe7fb318a\", \"resnet50_clothing1m.pth\", 94471714)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_651812d3-925a-4767-b404-6ce98de8050c\", \"resnet50_experiment\", 4096)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(model_path)\n",
        "files.download(writer_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HUVAaJ2Uv0u"
      },
      "source": [
        "The training has ended after epoch 2 because the laptop was offline. Here I continue training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQmRa9m8U4fz"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(epoch, model, optimizer, scheduler, best_val_loss, epochs_no_improve, checkpoint_path):\n",
        "    checkpoint_path = os.path.join(checkpoint_path, f'checkpoint_epoch_{epoch}.pth')\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'best_val_loss': best_val_loss,\n",
        "        'epochs_no_improve': epochs_no_improve\n",
        "    }, checkpoint_path)\n",
        "    print(f'Checkpoint saved at epoch {epoch}')\n",
        "\n",
        "def load_checkpoint(model, optimizer, scheduler, checkpoint_path):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    return checkpoint['epoch'], checkpoint['best_val_loss'], checkpoint['epochs_no_improve']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPcECIQyU-OX"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from config import num_epochs, dataset_root, checkpoint_path\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "# Initialize TensorBoard writer\n",
        "writer_path = dataset_root.replace(\"..\", \"/content\") + 'runs/resnet50_experiment'\n",
        "writer = SummaryWriter(writer_path)\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 5\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "early_stop = False\n",
        "\n",
        "# Load specific checkpoint if available\n",
        "start_epoch = 2\n",
        "current_checkpoint_path = os.path.join(checkpoint_path, f'checkpoint_epoch_{start_epoch}.pth')\n",
        "if os.path.exists(current_checkpoint_path):\n",
        "    start_epoch, best_val_loss, epochs_no_improve = load_checkpoint(model, optimizer, scheduler, current_checkpoint_path)\n",
        "    start_epoch += 1  # start from the next epoch\n",
        "    print(f'Resumed from checkpoint {current_checkpoint_path}')\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(start_epoch, num_epochs):\n",
        "    if early_stop:\n",
        "        break\n",
        "\n",
        "    model.train()  # Set model to training mode\n",
        "    epoch_start_time = time.time()  # Start time for the epoch\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        # Move input and label tensors to the device\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Zero out the optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        correct_predictions += calculate_accuracy(outputs, labels)\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    epoch_duration = time.time() - epoch_start_time  # End time for the epoch\n",
        "    avg_loss = running_loss / len(train_loader)  # Average loss for the epoch\n",
        "    accuracy = correct_predictions / total_samples  # Accuracy for the epoch\n",
        "\n",
        "    # Log the training loss, accuracy, and duration to TensorBoard\n",
        "    writer.add_scalar('Loss/train', avg_loss, epoch)\n",
        "    writer.add_scalar('Accuracy/train', accuracy, epoch)\n",
        "    writer.add_scalar('Time/train', epoch_duration, epoch)\n",
        "\n",
        "    # Validate the model\n",
        "    val_loss, val_accuracy = evaluate_model(val_loader, model, criterion, device)\n",
        "    writer.add_scalar('Loss/val', val_loss, epoch)\n",
        "    writer.add_scalar('Accuracy/val', val_accuracy, epoch)\n",
        "\n",
        "    # Print the loss, accuracy, and time for every epoch\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "          f'Train Loss: {avg_loss:.4f}, Train Accuracy: {accuracy:.4f}, '\n",
        "          f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, '\n",
        "          f'Time: {epoch_duration:.2f} sec')\n",
        "\n",
        "    # Check for early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        if epochs_no_improve >= patience:\n",
        "            print(f'Early stopping at epoch {epoch+1}')\n",
        "            early_stop = True\n",
        "\n",
        "    # Save checkpoint\n",
        "    save_checkpoint(epoch, model, optimizer, scheduler, best_val_loss, epochs_no_improve, checkpoint_path)\n",
        "\n",
        "    # Step the scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "print(f'Finished Training, Final Train Loss: {avg_loss:.4f}, Final Train Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Test the model\n",
        "test_loss, test_accuracy = evaluate_model(test_loader, model, criterion, device)\n",
        "writer.add_scalar('Loss/test', test_loss, 0)\n",
        "writer.add_scalar('Accuracy/test', test_accuracy, 0)\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "# Close the TensorBoard writer\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmSUgY5LJkr1"
      },
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFfLgWbug0-b"
      },
      "source": [
        "Evaluate models on validation data with accuracy, precision, recall, F1 score"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
